{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b80da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hashmat.malik/.conda/envs/advdrop_env/lib/python3.7/site-packages/robustness/train.py:24: UserWarning: Could not import amp.\n",
      "  warnings.warn('Could not import amp.')\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = '/tmp/'\n",
    "\n",
    "from torchvision import models\n",
    "import torchvision.datasets as dsets\n",
    "import json\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CIFAR, ImageNet\n",
    "import torch\n",
    "from cox.utils import Parameters\n",
    "import cox.store\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import os\n",
    "# PyTorch\n",
    "import torch.nn as nn\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188bd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_idx = json.load(open(\"/home/hashmat.malik/PycharmProjects/Thesis/AdvDrop/imagenet_class_index.json\"))\n",
    "idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "class2label = [class_idx[str(k)][0] for k in range(len(class_idx))]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), ])\n",
    "\n",
    "\n",
    "def image_folder_custom_label(root, transform, idx2label) :\n",
    "    \n",
    "    \n",
    "    old_data = dsets.ImageFolder(root=root, transform=transform)\n",
    "    old_classes = old_data.classes\n",
    "    \n",
    "    label2idx = {}\n",
    "    \n",
    "    for i, item in enumerate(idx2label) :\n",
    "        label2idx[item] = i\n",
    "    \n",
    "    new_data = dsets.ImageFolder(root=root, transform=transform, \n",
    "                                 target_transform=lambda x : idx2label.index(old_classes[x]))\n",
    "    new_data.classes = idx2label\n",
    "    new_data.class_to_idx = label2idx\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def create_dir(dir, print_flag = False):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "        if print_flag:\n",
    "            print(\"Create dir {} successfully!\".format(dir))\n",
    "    elif print_flag:\n",
    "        print(\"Directory {} is already existed. \".format(dir))\n",
    "\n",
    "\n",
    "def get_data(data_dir, transform, batch_size, class2label):\n",
    "\n",
    "    normal_data = image_folder_custom_label(root=data_dir,\n",
    "                                            transform=transform,\n",
    "                                            idx2label=class2label)\n",
    "    normal_loader = torch.utils.data.DataLoader(normal_data,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False)\n",
    "\n",
    "    return normal_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f70653",
   "metadata": {},
   "source": [
    "## Load VGG Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d4a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/home/hashmat/Thesis/AdvDrop/pretrained-models/vgg16_bn_l2_eps0.ckpt'\n",
      "=> loaded checkpoint '/home/hashmat/Thesis/AdvDrop/pretrained-models/vgg16_bn_l2_eps0.ckpt' (epoch 89)\n"
     ]
    }
   ],
   "source": [
    "imagenet_ds = ImageNet('/tmp/')\n",
    "vgg , _ = model_utils.make_and_restore_model(arch=models.vgg16_bn(), dataset=imagenet_ds, \n",
    "                                resume_path='/home/hashmat.malik/PycharmProjects/Thesis/AdvDrop/pretrained-models/vgg16_bn_l2_eps0.ckpt',\n",
    "                                parallel=False, add_custom_forward=True)\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e082d",
   "metadata": {},
   "source": [
    "## Load MobileNet-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_ds = ImageNet('/tmp/')\n",
    "mobilenet , _ = model_utils.make_and_restore_model(arch=models.mobilenet_v2(), dataset=imagenet_ds, \n",
    "                                resume_path='/home/hashmat.malik/PycharmProjects/Thesis/AdvDrop/pretrained-models/mobilenet_l2_eps0.ckpt',\n",
    "                                parallel=False, add_custom_forward=True)\n",
    "model = mobilenet.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62994789",
   "metadata": {},
   "source": [
    "## Load ResNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87562cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/home/hashmat.malik/PycharmProjects/Thesis/AdvDrop/pretrained-models/resnet18_l2_eps0.ckpt'\n",
      "=> loaded checkpoint '/home/hashmat.malik/PycharmProjects/Thesis/AdvDrop/pretrained-models/resnet18_l2_eps0.ckpt' (epoch 90)\n"
     ]
    }
   ],
   "source": [
    "imagenet_ds = ImageNet('/tmp/')\n",
    "resnet , _ = model_utils.make_and_restore_model(arch=\"resnet18\", dataset=imagenet_ds, \n",
    "                                resume_path='/home/hashmat.malik/PycharmProjects/Thesis/AdvDrop/pretrained-models/resnet18_l2_eps0.ckpt',\n",
    "                                parallel=False, add_custom_forward=True)\n",
    "model = resnet.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "#from Models.transformers import diet_tiny, diet_small, vit_tiny, vit_small\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean))\n",
    "        self.register_buffer('std', torch.Tensor(std))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Broadcasting\n",
    "        input = input / 255.0\n",
    "        mean = self.mean.reshape(1, 3, 1, 1)\n",
    "        std = self.std.reshape(1, 3, 1, 1)\n",
    "        return (input - mean) / std\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.Resize((224, 224)),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                   std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, bs=128, \n",
    "                 data_path=\"/home/hashmat.malik/PycharmProjects/Thesis/nobox-attacks/data/ILSVRC2012_img_val\",\n",
    "                     tf = transform):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    OUT_DIR = '/tmp/'\n",
    "    imagenet_ds = ImageNet('/tmp/')\n",
    "    model = model.eval()\n",
    "    batch_size = bs\n",
    "    data_dir = data_path\n",
    "    normal_loader = get_data(data_dir, tf, batch_size, class2label)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for images, labels in normal_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        preds = model(images)\n",
    "        _, pre = torch.max(preds, 1)\n",
    "        correct += torch.sum(pre==labels)\n",
    "        total += images.shape[0]\n",
    "        print(total)\n",
    "    print(f\"The Model accuracy is {correct/total}\")\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298a23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
